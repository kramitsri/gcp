{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Question Answering with Large Documents using LlamaIndex on Google cloud\n",
        "LlamaIndex is a powerful framework that simplifies the process of building RAG (Retrieval Augmented Generation) applications, especially for question answering with large documents. LlamaIndex simplifies the development process, while Vertex AI provides access to high-performing LLMs and scalable infrastructure. This combination enables you to create sophisticated AI applications that can effectively leverage external knowledge sources to provide accurate and informative responses.\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_to_llamaindex_rag.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/orchestration/intro_to_llamaindex_rag.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/orchestration/intro_to_llamaindex_rag.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGidY33I0-J2"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Mona Mona](https://github.com/mona19) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to use llamaindex\n",
        "\n",
        "- Deploy Vertex AI vector search index\n",
        "- Load document for question answering\n",
        "- Chunk and embed documents using Vertex AI embeddings into Vector Search index.\n",
        "- Use Gemini model to ask questions to the index\n",
        "- Evalute faithfulness of the response\n",
        "- Cleanup- delete the Vertex AI Vector Store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uSGoyR6RrTQ"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install Vertex AI SDK for LLamaIndex, other packages and their dependencies\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tvCecNMQpXk"
      },
      "outputs": [],
      "source": [
        "! pip install llama-index llama-index-vector-stores-vertexaivectorsearch llama-index-llms-vertex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yStiAHorWE3Q"
      },
      "source": [
        "***Colab only***: Run the following cell to restart the kernel or use the button to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opUxT_k5TdgP"
      },
      "source": [
        "### Authenticating your notebook environment\n",
        "\n",
        "- If you are using **Colab** to run this notebook, run the cell below and continue.\n",
        "- If you are using **Vertex AI Workbench**, check out the setup instructions [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbNgv4q1T2Mi"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvb4hvFl0-J6"
      },
      "source": [
        "- If you are running this notebook in a local development environment:\n",
        "  - Install the [Google Cloud SDK](https://cloud.google.com/sdk).\n",
        "  - Obtain authentication credentials. Create local credentials by running the following command and following the oauth2 flow (read more about the command [here](https://cloud.google.com/sdk/gcloud/reference/beta/auth/application-default/login)):\n",
        "\n",
        "    ```bash\n",
        "    gcloud auth application-default login\n",
        "    ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j7z-Hs6BJkD"
      },
      "source": [
        "We recommend you to follow the notebook code and instructions to setup below:\n",
        "Install LLamaIndex, authenticate your notebook and import necessary libraries using the instructions provided in the notebook.\n",
        "\n",
        " Create Vertex AI Vector Search  Index and deploy it to an Endpoint.Vertex AI Vector Search is a fully managed, scalable Google Cloud service designed for high-speed similarity searches across large datasets of high-dimensional vectors, crucial for various AI applications like recommendation systems and semantic search. This step can take 30 minutes. You should complete both of these tasks before moving to the next step.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6aYWDwo0-J6"
      },
      "source": [
        "**Colab only:** Run the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjSsu6cmUdEx"
      },
      "outputs": [],
      "source": [
        "# Project and Storage Constants\n",
        "PROJECT_ID = \"<>\"\n",
        "REGION = \"us-central1\"\n",
        "GCS_BUCKET_NAME = \"your bucket name\"\n",
        "GCS_BUCKET_URI = \"gs://your bucket name\"\n",
        "\n",
        "# The number of dimensions for the textembedding-gecko@003 is 768\n",
        "# If other embedder is used, the dimensions would probably need to change.\n",
        "VS_DIMENSIONS = 768\n",
        "\n",
        "# Vertex AI Vector Search Index configuration\n",
        "# parameter description here\n",
        "# https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex#google_cloud_aiplatform_MatchingEngineIndex_create_tree_ah_index\n",
        "VS_INDEX_NAME = \"llamaindex-doc-index\"  # @param {type:\"string\"}\n",
        "VS_INDEX_ENDPOINT_NAME = \"llamaindex-doc-endpoint\"\n",
        "\n",
        "\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)  # @param {type:\"string\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "# Create a bucket.\n",
        "! gsutil mb -l REGION−pREGION−pREGION -p PROJECT_ID $GCS_BUCKET_URI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVPDVkHq5J95"
      },
      "outputs": [],
      "source": [
        "# NOTE : This operation can take upto 30 seconds\n",
        "\n",
        "# check if index exists\n",
        "index_names = [\n",
        "    index.resource_name\n",
        "    for index in aiplatform.MatchingEngineIndex.list(\n",
        "        filter=f\"display_name={VS_INDEX_NAME}\"\n",
        "    )\n",
        "]\n",
        "\n",
        "if len(index_names) == 0:\n",
        "    print(f\"Creating Vector Search index {VS_INDEX_NAME} ...\")\n",
        "    vs_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "        display_name=VS_INDEX_NAME,\n",
        "        dimensions=VS_DIMENSIONS,\n",
        "        distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
        "        shard_size=\"SHARD_SIZE_SMALL\",\n",
        "        index_update_method=\"STREAM_UPDATE\",\n",
        "        approximate_neighbors_count=100,  # allowed values BATCH_UPDATE , STREAM_UPDATE\n",
        "    )\n",
        "    print(\n",
        "        f\"Vector Search index {vs_index.display_name} created with resource name {vs_index.resource_name}\"\n",
        "    )\n",
        "else:\n",
        "    vs_index = aiplatform.MatchingEngineIndex(index_name=index_names[0])\n",
        "    print(\n",
        "        f\"Vector Search index {vs_index.display_name} exists with resource name {vs_index.resource_name}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOLQBOT26Ow-"
      },
      "source": [
        "### Create a Vertex AI Vector Search Endpoint\n",
        "To use the index, you need to create an index endpoint. It works as a server instance accepting query requests for your index. An endpoint can be a public endpoint or a private endpoint.\n",
        "\n",
        "Let's create a public endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS8s5lGH6SNE"
      },
      "outputs": [],
      "source": [
        "endpoint_names = [\n",
        "    endpoint.resource_name\n",
        "    for endpoint in aiplatform.MatchingEngineIndexEndpoint.list(\n",
        "        filter=f\"display_name={VS_INDEX_ENDPOINT_NAME}\"\n",
        "    )\n",
        "]\n",
        "\n",
        "if len(endpoint_names) == 0:\n",
        "    print(f\"Creating Vector Search index endpoint {VS_INDEX_ENDPOINT_NAME} ...\")\n",
        "    vs_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "        display_name=VS_INDEX_ENDPOINT_NAME, public_endpoint_enabled=True\n",
        "    )\n",
        "    print(\n",
        "        f\"Vector Search index endpoint {vs_endpoint.display_name} created with resource name {vs_endpoint.resource_name}\"\n",
        "    )\n",
        "else:\n",
        "    vs_endpoint = aiplatform.MatchingEngineIndexEndpoint(\n",
        "        index_endpoint_name=endpoint_names[0]\n",
        "    )\n",
        "    print(\n",
        "        f\"Vector Search index endpoint {vs_endpoint.display_name} exists with resource name {vs_endpoint.resource_name}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JK7q2Cg9DwZ"
      },
      "source": [
        "**Deploy Index to the Endpoint¶**\n",
        "With the index endpoint, deploy the index by specifying a unique deployed index ID.\n",
        "\n",
        "NOTE : This operation can take upto 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pNiYWCd68wP"
      },
      "outputs": [],
      "source": [
        "# check if endpoint exists\n",
        "index_endpoints = [\n",
        "    (deployed_index.index_endpoint, deployed_index.deployed_index_id)\n",
        "    for deployed_index in vs_index.deployed_indexes\n",
        "]\n",
        "\n",
        "if len(index_endpoints) == 0:\n",
        "    print(\n",
        "        f\"Deploying Vector Search index {vs_index.display_name} at endpoint {vs_endpoint.display_name} ...\"\n",
        "    )\n",
        "    vs_deployed_index = vs_endpoint.deploy_index(\n",
        "        index=vs_index,\n",
        "        deployed_index_id=\"new_deployed_index_id\",\n",
        "        display_name=VS_INDEX_NAME,\n",
        "        machine_type=\"e2-standard-16\",\n",
        "        min_replica_count=1,\n",
        "        max_replica_count=1,\n",
        "    )\n",
        "    print(\n",
        "        f\"Vector Search index {vs_index.display_name} is deployed at endpoint {vs_deployed_index.display_name}\"\n",
        "    )\n",
        "else:\n",
        "    vs_deployed_index = aiplatform.MatchingEngineIndexEndpoint(\n",
        "        index_endpoint_name=index_endpoints[0][0]\n",
        "    )\n",
        "    print(\n",
        "        f\"Vector Search index {vs_index.display_name} is already deployed at endpoint {vs_deployed_index.display_name}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdsO2dZQLLuv"
      },
      "outputs": [],
      "source": [
        "# import modules needed\n",
        "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
        "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
        "from llama_index.llms.vertex import Vertex\n",
        "from llama_index.vector_stores.vertexaivectorsearch import VertexAIVectorStore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8GO1JKtK347"
      },
      "source": [
        "**Parse, Index and Query PDFs using Vertex AI Vector Search and Gemini Pro**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1CZIsNNKrB-"
      },
      "outputs": [],
      "source": [
        "! mkdir -p ./data/arxiv/\n",
        "! wget 'https://arxiv.org/pdf/1706.03762.pdf' -O ./data/arxiv/test.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6ISbiIvC6S3"
      },
      "source": [
        "### Ingesting documents\n",
        "In this step, we will be using the LlamaIndex utility class called SimpleDirectoryReader. This can  easily load and parse various file types from a local directory. It automatically handles different formats, extracts metadata, and can be read recursively.\n",
        "\n",
        "The following code creates a directory structure \"./data/arxiv/\" using the mkdir command with the -p flag to ensure all parent directories are created. It then downloads the PDF of the \"Attention Is All You Need\" paper from arXiv using wget command, saving it as \"test.pdf\" in the new directory. Next, we are using SimpleDirectoryReader from LlamaIndex  to read the contents of the \"./data/arxiv/\" directory and load the documents. Finally, it prints the number of documents loaded, which should be 1 for the downloaded PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdybUAVyK_Qm"
      },
      "outputs": [],
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(\"./data/arxiv/\").load_data()\n",
        "print(f\"# of documents = {len(documents)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVsUeOLoK1RC"
      },
      "source": [
        "! mkdir -p ./data/arxiv/\n",
        "! wget 'https://arxiv.org/pdf/1706.03762.pdf' -O ./data/arxiv/test.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAGaTjPVTmhP"
      },
      "source": [
        "### Import models and intiatilze Vector Store\n",
        "\n",
        "You load the pre-trained text and embeddings generation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITUmZiNZcMUW"
      },
      "outputs": [],
      "source": [
        "# setup storage\n",
        "vector_store = VertexAIVectorStore(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=REGION,\n",
        "    index_id=vs_index.resource_name,\n",
        "    endpoint_id=vs_endpoint.resource_name,\n",
        "    gcs_bucket_name=GCS_BUCKET_NAME,\n",
        ")\n",
        "\n",
        "# set storage context\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# configure embedding model\n",
        "embed_model = VertexTextEmbedding(\n",
        "    model_name=\"textembedding-gecko@003\",\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        ")\n",
        "\n",
        "vertex_gemini = Vertex(model=\"gemini-pro\", temperature=0, additional_kwargs={})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woEzfeYyDcE7"
      },
      "source": [
        "###Create embeddings and storing embeddings in Vertex AI Vector search\n",
        "\n",
        "We are using VectorStoreIndex.from_documents() from LlamaIndex which creates a vector index from the given documents, using the specified storage context from Vertex AI vector search and Vertex AI embedding model created in the previous step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "268jlr_RPK8R"
      },
      "outputs": [],
      "source": [
        "# define index from vector store\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context, embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEoWdGFoDk5I"
      },
      "source": [
        "###Using LlamaIndex query engine to query\n",
        "\n",
        "Now we  construct an query_engine object from your LlamaIndex index. The query_engine is responsible for processing user queries, retrieving relevant information from the index, and generating responses using the specified LLM which is Vertex AI Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUzqXWwPPNLr"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU03APMkfdPP"
      },
      "source": [
        "**Set up Query engine with Gemini **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zqk19GPSAHW"
      },
      "outputs": [],
      "source": [
        "llm = vertex_gemini\n",
        "query_engine = index.as_query_engine(\n",
        "    llm=llm,\n",
        "    similarity_top_k=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42c95EKkQOIa"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"who are the authors of paper Attention is All you need?\")\n",
        "\n",
        "print(\"Response:\")\n",
        "print(\"-\" * 80)\n",
        "print(response.response)\n",
        "print(\"-\" * 80)\n",
        "print(\"Source Documents:\")\n",
        "print(\"-\" * 80)\n",
        "for source in response.source_nodes:\n",
        "    print(f\"Sample Text: {source.text[:50]}\")\n",
        "    print(f\"Relevance score: {source.get_score():.3f}\")\n",
        "    print(f\"File Name: {source.metadata.get('file_name')}\")\n",
        "    print(f\"Page #: {source.metadata.get('page_label')}\")\n",
        "    print(f\"File Path: {source.metadata.get('file_path')}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHuerdrDVizO"
      },
      "source": [
        "**Response Evaluation**\n",
        "\n",
        "Does the response match the retrieved context? Does it also match the query? Does it match the reference answer or guidelines? Here's a simple example that evaluates a single response for Faithfulness, i.e. whether the response is aligned to the context, such as being free from hallucinations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UAjC_IlQVI6"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFg3jfLAUdB9"
      },
      "outputs": [],
      "source": [
        "# define evaluator\n",
        "evaluator = FaithfulnessEvaluator(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwftSatTVXE1"
      },
      "outputs": [],
      "source": [
        "# query index\n",
        "!pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfnbuqoDVv8K"
      },
      "source": [
        "The response contains both the response and the source from which the response was generated; the evaluator compares them and determines if the response is faithful to the source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAHgR44dUqb0"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "response = query_engine.query(\"who are the authors of paper Attention is All you need?\")\n",
        "eval_result = evaluator.evaluate_response(response=response)\n",
        "print(str(eval_result.passing))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFzWQKD3Vs3Z"
      },
      "source": [
        "**Clean Up**\n",
        "\n",
        "Please delete Vertex AI Vector Search Index and Index Endpoint after running your experiments to avoid incurring additional charges. Please note that you will be charged as long as the endpoint is running.\n",
        "\n",
        "⚠️ NOTE: Enabling `CLEANUP_RESOURCES` flag deletes Vector Search Index, Index Endpoint and Cloud Storage bucket. Please run it with caution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj0LWj8AcATe"
      },
      "outputs": [],
      "source": [
        "CLEANUP_RESOURCES = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNlzx-Nlcf-W"
      },
      "source": [
        "Undeploy indexes and Delete index endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swr3kHGycc2S"
      },
      "outputs": [],
      "source": [
        "if CLEANUP_RESOURCES:\n",
        "    print(\n",
        "        f\"Undeploying all indexes and deleting the index endpoint {vs_endpoint.display_name}\"\n",
        "    )\n",
        "    vs_endpoint.undeploy_all()\n",
        "    vs_endpoint.delete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaxY7_qxckTZ"
      },
      "outputs": [],
      "source": [
        "if CLEANUP_RESOURCES:\n",
        "    print(f\"Deleting the index {vs_index.display_name}\")\n",
        "    vs_index.delete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cv5ZZvUucpK6"
      },
      "outputs": [],
      "source": [
        "if CLEANUP_RESOURCES and \"GCS_BUCKET_NAME\" in globals():\n",
        "    print(f\"Deleting contents from the Cloud Storage bucket {GCS_BUCKET_NAME}\")\n",
        "\n",
        "    shell_output = ! gsutil du -ash gs://GCS_BUCKET_NAME    print(shell_output)    print(        f\"Size of the bucket {GCS_BUCKET_NAME} before deleting = {' '.join(shell_output[0].split()[:2])}\"    )    # uncomment below line to delete contents of the bucket    # ! gsutil -m rm -r gs://GCS_BUCKET_NAME    print(shell_output)    print(        f\"Size of the bucket {GCS_BUCKET_NAME} before deleting = {' '.join(shell_output[0].split()[:2])}\"    )    # uncomment below line to delete contents of the bucket    # ! gsutil -m rm -r gs://GCS_BUCKET_NAME\n",
        "    print(shell_output)\n",
        "    print(\n",
        "        f\"Size of the bucket {GCS_BUCKET_NAME} before deleting = {' '.join(shell_output[0].split()[:2])}\"\n",
        "    )\n",
        "\n",
        "    # uncomment below line to delete contents of the bucket\n",
        "    # ! gsutil -m rm -r gs://GCS_BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Intro_to_LlamaIndex_RAG.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
